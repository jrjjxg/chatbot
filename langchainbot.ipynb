{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4991244a-d8b0-48ef-81a5-04d56183ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_core langgraph langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c11988-add2-4a76-8bbb-478bf2b75cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-973abc04ec464088aabcd5a5dfcfe3b0\n",
      "https://api.deepseek.com/v1\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 配置API设置\n",
    "api_key = os.environ[\"DEEPSEEK_API_KEY\"]\n",
    "api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "\n",
    "# 在创建ChatOpenAI实例时使用这些设置\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",  # DeepSeek的模型名称\n",
    "    openai_api_key=api_key,\n",
    "    openai_api_base=api_base,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 打印API设置\n",
    "print(api_key)\n",
    "print(api_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15f898a3-b461-4f00-ac00-1b442f844b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi Bob! 👋 Nice to meet you—I’m DeepSeek Chat. How can I help you today? 😊', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 8, 'total_tokens': 35, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 8}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_3d5141a69a_prod0225', 'id': '5ce60ea3-d6a2-40b1-8c0e-b54d77f5d6e2', 'finish_reason': 'stop', 'logprobs': None}, id='run-94aef685-3036-491e-b8a9-cf365f2fd793-0', usage_metadata={'input_tokens': 8, 'output_tokens': 27, 'total_tokens': 35, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02eac358-1da9-4739-90e9-83e7cdbb2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "#MessagesState内有一个messages字段，用于存储消息，messages是一个列表，列表中每个元素是一个Message对象\n",
    "class State(MessagesState):\n",
    "    #state继承MessagesState\n",
    "    #state中有一个summary字段，用于存储摘要\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "215cb9fc-a85f-4355-b4dd-6102ff385348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logic to call the model\n",
    "def call_model(state: State):\n",
    "    \n",
    "    # Get summary if it exists\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # If there is summary, then we add it\n",
    "    if summary:\n",
    "        \n",
    "        # Add summary to system message\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "        \n",
    "        # Append summary to any newer messages\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    \n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a6784c9-131c-4688-b643-90e69d8957f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_conversation(state: State):\n",
    "    \n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt \n",
    "    if summary:\n",
    "        \n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    #上面是创建一个总结的prompt，然后将prompt添加到messages中\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ad0bd2c-afda-42f6-9b5e-afa404d47a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_continue(state: State):\n",
    "    \n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    # Otherwise we can just end\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee1eeb7f-8e38-4c99-9043-19e903152c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ec98853-b69c-4757-b46e-2e8828973ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Lance! 👋 Nice to meet you—what’s on your mind today? Whether it’s questions, random thoughts, or just saying hello, I’m here for it. 😊  \n",
      "\n",
      "(Also, love the name—are you a fan of knights, spears, or perhaps a certain Pokémon trainer? �⚔️🐉)\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is **Lance**! Unless you’ve secretly switched it in the last few minutes—then you’ll have to fill me in. 😉  \n",
      "\n",
      "(Still holding out hope you’re either a knight or a dragon tamer, though. 🏰🐲)\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Nice! The **49ers** are a legendary franchise—great choice! 🏈💛❤️  \n",
      "\n",
      "Are you hyped for this season? Any favorite players (past or present)? Deebo’s explosiveness? Kittle’s energy? Or maybe you’re still riding the high of the Montana/Young era?  \n",
      "\n",
      "Also, how do you feel about the NFC West rivalry drama? (Looking at you, Seahawks and Rams. 👀)  \n",
      "\n",
      "Let’s talk football! 🎉\n"
     ]
    }
   ],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "input_message = HumanMessage(content=\"what's my name?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "input_message = HumanMessage(content=\"i like the 49ers!\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d9d4fd9-6dab-4f61-87a0-f996c03d83e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).values.get(\"summary\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e8ada79-8f0e-4299-9868-aeb8398cce8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You're absolutely right! **Nick Bosa** is a *monster* on the field and now officially the **highest-paid defensive player in NFL history**! 🏈💰  \n",
      "\n",
      "After his 2022 **DPOY** season and holding out for a mega-deal, the 49ers locked him down in 2023 with a **5-year, $170 million contract** ($34M/year), including **$122.5 million guaranteed**. That dethroned Aaron Donald’s previous record.  \n",
      "\n",
      "**Why he’s worth it:**  \n",
      "- Elite pass rusher (18.5 sacks in ‘22, led the league)  \n",
      "- Disrupts offenses *every* snap (double-team magnet)  \n",
      "- Clutch in big games (see: 2023 playoffs)  \n",
      "\n",
      "The only question now: Can he and that stacked D-line bring the Niners a **Super Bowl** this season? 🔥  \n",
      "\n",
      "*(Also, how badly do you miss his brother Joey in the Chargers games? The Bosa family genes are unfair. 😂)*\n"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage(content=\"i like Nick Bosa, isn't he the highest paid defensive player?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02add9f7-7d5f-4533-ba7c-bce6e19af9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### **Summary of Our Conversation:**  \\n\\n1. **Introduction:** You introduced yourself as **Lance**, and I playfully guessed if your name was inspired by knights, spears, or Pokémon (Lance the Dragon Trainer).  \\n\\n2. **49ers Fandom:** You mentioned loving the **San Francisco 49ers**, and we discussed:  \\n   - Your favorite player, **Nick Bosa**, and his record-breaking **$170M contract** (highest-paid defensive player ever).  \\n   - His dominance (18.5 sacks in 2022, DPOY, elite pass-rushing).  \\n   - Hopes for a **Super Bowl run** with the 49ers' stacked defense.  \\n   - Lighthearted jokes about the **Bosa family genes** (shoutout to Joey Bosa).  \\n\\n3. **NFC West Rivalries:** Briefly touched on the **Seahawks and Rams drama**.  \\n\\n**Theme:** A mix of fun banter, football hype, and appreciation for Nick Bosa’s greatness! 🏈💪  \\n\\nLet me know if you want to dive deeper into anything!\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).values.get(\"summary\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a826c7a-d64a-4952-b1e4-14a3b40ed214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchainbot)",
   "language": "python",
   "name": "langchain-learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
